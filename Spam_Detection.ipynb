{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRsHRj99Vz0x2MujkjiS83",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sathwikadonagani/Assignments-Jala-Academy-/blob/main/Spam_Detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ru-AAeyVB7fH",
        "outputId": "fae69286-7c70-4d03-ca6f-9d522e3eaf73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [08:42:38] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:199: UserWarning: [08:44:25] WARNING: /workspace/src/learner.cc:790: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üî¨ Novel Model: Semantic-Weighted XGBoost with Custom Learning Functions\n",
            "\n",
            "Accuracy: 0.9757\n",
            "\n",
            "Confusion Matrix:\n",
            "[[7591  317]\n",
            " [  88 8694]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Ham (0)       0.99      0.96      0.97      7908\n",
            "    Spam (1)       0.96      0.99      0.98      8782\n",
            "\n",
            "    accuracy                           0.98     16690\n",
            "   macro avg       0.98      0.97      0.98     16690\n",
            "weighted avg       0.98      0.98      0.98     16690\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# Novel Spam Classification using XGBoost\n",
        "# with Custom Algorithmic Functions\n",
        "# (Research-Level Modification)\n",
        "# ======================================================\n",
        "\n",
        "import sys\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# -----------------------------\n",
        "# NLTK Setup\n",
        "# -----------------------------\n",
        "def setup_nltk():\n",
        "    try:\n",
        "        nltk.data.find('corpora/stopwords')\n",
        "    except LookupError:\n",
        "        nltk.download('stopwords')\n",
        "\n",
        "# -----------------------------\n",
        "# Text Preprocessing\n",
        "# -----------------------------\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "# =====================================================\n",
        "# üî¨ NEW RESEARCH FUNCTIONS (NOT PREDEFINED)\n",
        "# =====================================================\n",
        "\n",
        "# 1Ô∏è‚É£ Linguistic Saliency Function (LSF)\n",
        "def linguistic_saliency(text, alpha=0.6, beta=0.3, gamma=0.1):\n",
        "    spam_keywords = ['free', 'win', 'urgent', 'offer', 'limited', 'click', 'buy']\n",
        "    text = str(text).lower()\n",
        "\n",
        "    K = sum(text.count(word) for word in spam_keywords)   # keyword density\n",
        "    P = text.count('!') + text.count('?')                 # punctuation intensity\n",
        "    C = sum(1 for w in text.split() if w.isupper())       # capitalization\n",
        "\n",
        "    score = alpha * K + beta * P + gamma * C\n",
        "    return np.tanh(score)  # bounded confidence score (0 to 1)\n",
        "\n",
        "# 2Ô∏è‚É£ Adaptive Confidence Weighting Function (ACWF)\n",
        "def adaptive_confidence_weight(text, base=1.0, lam=0.8):\n",
        "    lsf = linguistic_saliency(text)\n",
        "    return base + lam * lsf\n",
        "\n",
        "# 3Ô∏è‚É£ Entropy-Guided Uncertainty Penalty (EGUP)\n",
        "def entropy_uncertainty(p):\n",
        "    eps = 1e-9\n",
        "    return - (p * np.log(p + eps) + (1 - p) * np.log(1 - p + eps))\n",
        "\n",
        "# 4Ô∏è‚É£ Semantic Weight Refinement Function (SWRF)\n",
        "def semantic_weight_refinement(base_weight, predicted_prob, mu=0.5):\n",
        "    entropy = entropy_uncertainty(predicted_prob)\n",
        "    return base_weight * (1 - mu * entropy)\n",
        "\n",
        "# -----------------------------\n",
        "# Load Data\n",
        "# -----------------------------\n",
        "def load_data(filepath):\n",
        "    df = pd.read_csv(filepath)\n",
        "    df.dropna(inplace=True)\n",
        "    df['label'] = df['label'].astype(int)\n",
        "    df['processed_text'] = df['text'].apply(preprocess_text)\n",
        "    return df\n",
        "\n",
        "# -----------------------------\n",
        "# Main Execution\n",
        "# -----------------------------\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    DATASET_PATH = 'combined_data.csv'\n",
        "\n",
        "    try:\n",
        "        setup_nltk()\n",
        "\n",
        "        # Load Dataset\n",
        "        df = load_data(DATASET_PATH)\n",
        "        X = df['processed_text']\n",
        "        y = df['label']\n",
        "        X_raw = df['text']\n",
        "\n",
        "        # Train-Test Split\n",
        "        X_train, X_test, y_train, y_test, X_train_raw, X_test_raw = train_test_split(\n",
        "            X, y, X_raw, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        # TF-IDF Vectorization\n",
        "        vectorizer = TfidfVectorizer(max_features=5000)\n",
        "        X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "        X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "        # =====================================================\n",
        "        # üîß APPLY NEW ALGORITHMIC FUNCTIONS\n",
        "        # =====================================================\n",
        "\n",
        "        # Step 1: Compute Adaptive Confidence Weights\n",
        "        base_weights = np.array([adaptive_confidence_weight(text) for text in X_train_raw])\n",
        "\n",
        "        # Initial Model Training\n",
        "        model = XGBClassifier(\n",
        "            objective='binary:logistic',\n",
        "            eval_metric='logloss',\n",
        "            use_label_encoder=False,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        model.fit(X_train_tfidf, y_train, sample_weight=base_weights)\n",
        "\n",
        "        # Step 2: Predict Probabilities\n",
        "        y_train_prob = model.predict_proba(X_train_tfidf)[:, 1]\n",
        "\n",
        "        # Step 3: Refine Weights using Entropy-Guided Penalty\n",
        "        refined_weights = np.array([\n",
        "            semantic_weight_refinement(w, p) for w, p in zip(base_weights, y_train_prob)\n",
        "        ])\n",
        "\n",
        "        # Step 4: Retrain with Semantic Gradient Scaling\n",
        "        model.fit(X_train_tfidf, y_train, sample_weight=refined_weights)\n",
        "\n",
        "        # =====================================================\n",
        "        # Evaluation\n",
        "        # =====================================================\n",
        "        y_pred = model.predict(X_test_tfidf)\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        report = classification_report(y_test, y_pred, target_names=['Ham (0)', 'Spam (1)'])\n",
        "\n",
        "        print(\"\\nüî¨ Novel Model: Semantic-Weighted XGBoost with Custom Learning Functions\")\n",
        "        print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "        print(\"\\nConfusion Matrix:\")\n",
        "        print(cm)\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(report)\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Dataset file not found at '{DATASET_PATH}'\")\n",
        "        print(\"Please update the dataset path.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n"
      ]
    }
  ]
}